{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2ddf7c3c42e4444b26ba64a98e1649c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b634332ccf445899aece61fc8be1529",
              "IPY_MODEL_48b1f7db7a79448196b30a86d6cbd6c8",
              "IPY_MODEL_294376b7c0ae45c6aa4dc055538102ae"
            ],
            "layout": "IPY_MODEL_c48fa1c156a3497ab92f116a2442755f"
          }
        },
        "7b634332ccf445899aece61fc8be1529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44ede8b0d42445dca12daa83b8847ca0",
            "placeholder": "​",
            "style": "IPY_MODEL_633911e38d0340f09f3bc5c9b001d903",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "48b1f7db7a79448196b30a86d6cbd6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc0115cf9462427982da4e747d5278b0",
            "max": 276,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a80b13188e2c4559bda98625bb427375",
            "value": 276
          }
        },
        "294376b7c0ae45c6aa4dc055538102ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dccb19ac2bae4ea193aaa9c7eea95589",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9bc549921a4d989b9646829e2fe350",
            "value": " 276/276 [00:00&lt;00:00, 9.76kB/s]"
          }
        },
        "c48fa1c156a3497ab92f116a2442755f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ede8b0d42445dca12daa83b8847ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633911e38d0340f09f3bc5c9b001d903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc0115cf9462427982da4e747d5278b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80b13188e2c4559bda98625bb427375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dccb19ac2bae4ea193aaa9c7eea95589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9bc549921a4d989b9646829e2fe350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Section"
      ],
      "metadata": {
        "id": "5rjf2gZ2HtzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EeYTtTTGyPk",
        "outputId": "cfc44ae8-3745-41d3-bbba-4233bb221d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader,Dataset, Subset\n",
        "from torchvision import models, transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, AutoImageProcessor, BeitModel\n",
        "\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMo3KJJcHEuv",
        "outputId": "dd0660e4-38b0-4f84-88bb-208db30c824c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Section"
      ],
      "metadata": {
        "id": "MqXWqJIfXL_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model architecture"
      ],
      "metadata": {
        "id": "pKilzPbTXats"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16_Beit(nn.Module):\n",
        "\n",
        "  def __init__(self, vgg16, beit):\n",
        "      super(VGG16_Beit, self).__init__()\n",
        "\n",
        "      #features\n",
        "      self.vgg16 = vgg16\n",
        "      self.beit = beit\n",
        "\n",
        "      #classifier\n",
        "      input_size = beit.config.hidden_size + vgg16.classifier[-1].out_features\n",
        "      self.fc1 = nn.Linear(input_size, 256)\n",
        "      self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "  def forward(self, vgg16_images, beit_images):\n",
        "\n",
        "    #features combination\n",
        "    vgg16_features = self.vgg16(vgg16_images)\n",
        "    beit_features = self.beit(beit_images)\n",
        "\n",
        "    combined_features = torch.cat([vgg16_features, beit_features.pooler_output], dim=1)\n",
        "\n",
        "    r1 = F.relu(self.fc1(combined_features))\n",
        "\n",
        "    #classify then\n",
        "    return self.fc2(r1)"
      ],
      "metadata": {
        "id": "FZhMRb7DHReC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load from google drive the pretrained version"
      ],
      "metadata": {
        "id": "HYJXrha-bMi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the entire model from the .pt file\n",
        "model = torch.load('drive/MyDrive/classifier0609242212.pt')\n",
        "\n",
        "#Send the model to GPU if available\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "VAhQtpABXN-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bf74c0-3708-4c38-8d33-dd1523fffa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-5e4f80b33722>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('drive/MyDrive/classifier0609242212.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Section"
      ],
      "metadata": {
        "id": "FH6X0dJQUJVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transform and Collate"
      ],
      "metadata": {
        "id": "9zVerVDibU0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define two transformations: one for VGG16 and one for BEiT model"
      ],
      "metadata": {
        "id": "BEqM-b7pbYFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VGG16 transform\n",
        "transform_vgg = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "#BEiT transform\n",
        "beit_image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n",
        "\n",
        "transform_beit = lambda image: torch.squeeze(beit_image_processor(image, return_tensors=\"pt\")['pixel_values'])"
      ],
      "metadata": {
        "id": "kRPBHV85h4vg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "a2ddf7c3c42e4444b26ba64a98e1649c",
            "7b634332ccf445899aece61fc8be1529",
            "48b1f7db7a79448196b30a86d6cbd6c8",
            "294376b7c0ae45c6aa4dc055538102ae",
            "c48fa1c156a3497ab92f116a2442755f",
            "44ede8b0d42445dca12daa83b8847ca0",
            "633911e38d0340f09f3bc5c9b001d903",
            "bc0115cf9462427982da4e747d5278b0",
            "a80b13188e2c4559bda98625bb427375",
            "dccb19ac2bae4ea193aaa9c7eea95589",
            "fe9bc549921a4d989b9646829e2fe350"
          ]
        },
        "outputId": "f1cad768-5b0a-43c2-9b66-a7c3509c1530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2ddf7c3c42e4444b26ba64a98e1649c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform function wrapper; it is needed because we want both the image and the label as params"
      ],
      "metadata": {
        "id": "BtFM1X4Qblz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_transform(image, label):\n",
        "  return {\n",
        "      'vgg16_image': transform_vgg(image),\n",
        "      'beit_image': transform_beit(image),\n",
        "      'label': label\n",
        "  }\n"
      ],
      "metadata": {
        "id": "XWdfytlfhpLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collate"
      ],
      "metadata": {
        "id": "TW5UznFHUnus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  vgg16_images = torch.stack([item['vgg16_image'] for item in batch])\n",
        "  beit_images = torch.stack([item['beit_image'] for item in batch])\n",
        "  labels = torch.tensor([item['label'] for item in batch])\n",
        "  return {'vgg16_images': vgg16_images, 'beit_images': beit_images, 'labels': labels}"
      ],
      "metadata": {
        "id": "Woo50RiaUmnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FFHQ Dataset"
      ],
      "metadata": {
        "id": "KseMNPYZHwa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper class for FFHQ dataset, ref to this [link](https://github.com/NVlabs/ffhq-dataset) for further details about this dataset"
      ],
      "metadata": {
        "id": "MBdQ8S6Ab4b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFHQ_Dataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.image_files = self._get_image_files(img_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in directory: {img_dir}\")\n",
        "\n",
        "    def _get_image_files(self, dir_path):\n",
        "          image_files = []\n",
        "          for root, _, files in os.walk(dir_path):\n",
        "              for file in files:\n",
        "                  if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                      image_files.append(os.path.join(root, file))\n",
        "          return image_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
        "        return self.transform(image, 0)  # Returning 0 as a dummy label since all images belong to the same class\n"
      ],
      "metadata": {
        "id": "30fQb8rSdx2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FFHQ dataset initialization"
      ],
      "metadata": {
        "id": "MvV1w05BcGPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffhq_dataset = FFHQ_Dataset(img_dir='drive/MyDrive/ComputerVision/datasets/ffhq-dataset/images1024x1024', transform=dataset_transform)"
      ],
      "metadata": {
        "id": "OiJtUn96iPrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6a5bb1-9b5f-4b71-adb3-490ca526c956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70000 images in directory: drive/MyDrive/ComputerVision/datasets/ffhq-dataset/images1024x1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sample"
      ],
      "metadata": {
        "id": "lmiLRrtacLUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffhq_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiHSPveEfcIU",
        "outputId": "0d37a30a-aac3-4320-e847-bc941f24e5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vgg16_image': tensor([[[-2.1179, -2.1179, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "          ...,\n",
              "          [-1.7412, -1.7240, -1.7412,  ...,  0.1426,  0.1426,  0.1426],\n",
              "          [-1.7925, -1.7754, -1.7754,  ...,  0.1768,  0.1768,  0.1426],\n",
              "          [-1.8439, -1.8439, -1.7925,  ...,  0.1254,  0.1939,  0.1597]],\n",
              " \n",
              "         [[ 0.2402,  0.2227,  0.2402,  ...,  0.0301,  0.0126, -0.0224],\n",
              "          [ 0.2227,  0.2227,  0.2227,  ..., -0.0224, -0.0399, -0.0399],\n",
              "          [ 0.2227,  0.2052,  0.1877,  ..., -0.0749, -0.0574, -0.0574],\n",
              "          ...,\n",
              "          [ 0.6429,  0.6604,  0.6779,  ...,  0.7129,  0.6954,  0.6604],\n",
              "          [ 0.5728,  0.6078,  0.6254,  ...,  0.7129,  0.6954,  0.6954],\n",
              "          [ 0.5553,  0.5553,  0.5553,  ...,  0.6779,  0.7304,  0.7129]],\n",
              " \n",
              "         [[ 0.7402,  0.7228,  0.7402,  ...,  0.8797,  0.8622,  0.8099],\n",
              "          [ 0.7576,  0.7228,  0.7228,  ...,  0.8274,  0.8274,  0.8274],\n",
              "          [ 0.7576,  0.7228,  0.6705,  ...,  0.8274,  0.8274,  0.8448],\n",
              "          ...,\n",
              "          [ 1.1411,  1.1411,  1.1759,  ...,  0.9494,  0.9842,  1.0191],\n",
              "          [ 1.1062,  1.1062,  1.1237,  ...,  1.0017,  1.0017,  1.0191],\n",
              "          [ 1.0714,  1.0888,  1.1237,  ...,  0.9494,  1.0365,  1.0365]]]),\n",
              " 'beit_image': tensor([[[-1.0000, -1.0000, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          ...,\n",
              "          [-0.8275, -0.8196, -0.8275,  ...,  0.0353,  0.0353,  0.0353],\n",
              "          [-0.8510, -0.8431, -0.8431,  ...,  0.0510,  0.0510,  0.0353],\n",
              "          [-0.8745, -0.8745, -0.8510,  ...,  0.0275,  0.0588,  0.0431]],\n",
              " \n",
              "         [[ 0.0196,  0.0118,  0.0196,  ..., -0.0745, -0.0824, -0.0980],\n",
              "          [ 0.0118,  0.0118,  0.0118,  ..., -0.0980, -0.1059, -0.1059],\n",
              "          [ 0.0118,  0.0039, -0.0039,  ..., -0.1216, -0.1137, -0.1137],\n",
              "          ...,\n",
              "          [ 0.2000,  0.2078,  0.2157,  ...,  0.2314,  0.2235,  0.2078],\n",
              "          [ 0.1686,  0.1843,  0.1922,  ...,  0.2314,  0.2235,  0.2235],\n",
              "          [ 0.1608,  0.1608,  0.1608,  ...,  0.2157,  0.2392,  0.2314]],\n",
              " \n",
              "         [[ 0.1451,  0.1373,  0.1451,  ...,  0.2078,  0.2000,  0.1765],\n",
              "          [ 0.1529,  0.1373,  0.1373,  ...,  0.1843,  0.1843,  0.1843],\n",
              "          [ 0.1529,  0.1373,  0.1137,  ...,  0.1843,  0.1843,  0.1922],\n",
              "          ...,\n",
              "          [ 0.3255,  0.3255,  0.3412,  ...,  0.2392,  0.2549,  0.2706],\n",
              "          [ 0.3098,  0.3098,  0.3176,  ...,  0.2627,  0.2627,  0.2706],\n",
              "          [ 0.2941,  0.3020,  0.3176,  ...,  0.2392,  0.2784,  0.2784]]]),\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split this dataset to the half of its length because it has got so much images to be computed with the free plan of colab"
      ],
      "metadata": {
        "id": "3RKCYTCbcPbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "half_size = len(ffhq_dataset) // 2\n",
        "ffhq_subset = Subset(ffhq_dataset, torch.randperm(len(ffhq_dataset))[:half_size])\n",
        "\n",
        "# Create DataLoader\n",
        "ffhq_dataloader = DataLoader(ffhq_subset, batch_size=64, shuffle=False, num_workers=8, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "NO_-uKKlJ59Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83910c3e-9718-42b3-d89e-70a434a675c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sample of subsampled FFHQ"
      ],
      "metadata": {
        "id": "LIXmIrBhcg-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffhq_subset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rd8e5_BrOB9",
        "outputId": "84c25f4c-0b40-4192-aac7-0bd24aacd2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vgg16_image': tensor([[[-0.6623, -0.6623, -0.6794,  ...,  2.2489,  2.2489,  2.2318],\n",
              "          [-0.6623, -0.6452, -0.6794,  ...,  2.2318,  2.2318,  2.2318],\n",
              "          [-0.5938, -0.5938, -0.6281,  ...,  2.2489,  2.2489,  2.2318],\n",
              "          ...,\n",
              "          [ 1.4269,  1.4612,  1.4098,  ..., -1.3987, -1.4329, -1.4672],\n",
              "          [ 1.4440,  1.4098,  1.4098,  ..., -1.4500, -1.4158, -1.3987],\n",
              "          [ 1.4612,  1.3755,  1.3927,  ..., -1.4329, -1.4843, -1.3644]],\n",
              " \n",
              "         [[-0.8452, -0.8452, -0.8452,  ...,  1.9384,  1.8508,  1.8683],\n",
              "          [-0.8102, -0.7752, -0.8102,  ...,  1.9034,  1.9034,  1.9909],\n",
              "          [-0.7752, -0.7752, -0.7927,  ...,  2.0434,  2.0259,  2.0259],\n",
              "          ...,\n",
              "          [ 1.3256,  1.3606,  1.2906,  ..., -1.5105, -1.4930, -1.5105],\n",
              "          [ 1.3256,  1.3081,  1.2906,  ..., -1.5630, -1.4580, -1.4755],\n",
              "          [ 1.3606,  1.2731,  1.2906,  ..., -1.5455, -1.5805, -1.4755]],\n",
              " \n",
              "         [[-0.8633, -0.8458, -0.9156,  ...,  1.6465,  1.5594,  1.6465],\n",
              "          [-0.8284, -0.8284, -0.8807,  ...,  1.6640,  1.6465,  1.7685],\n",
              "          [-0.8284, -0.8284, -0.8633,  ...,  1.8383,  1.8034,  1.8383],\n",
              "          ...,\n",
              "          [ 1.1411,  1.1237,  1.0888,  ..., -1.4907, -1.5430, -1.5953],\n",
              "          [ 1.1585,  1.0888,  1.0714,  ..., -1.5430, -1.4907, -1.5081],\n",
              "          [ 1.1411,  1.0191,  1.0539,  ..., -1.4907, -1.5081, -1.3861]]]),\n",
              " 'beit_image': tensor([[[-0.3333, -0.3333, -0.3412,  ...,  1.0000,  1.0000,  0.9922],\n",
              "          [-0.3333, -0.3255, -0.3412,  ...,  0.9922,  0.9922,  0.9922],\n",
              "          [-0.3020, -0.3020, -0.3176,  ...,  1.0000,  1.0000,  0.9922],\n",
              "          ...,\n",
              "          [ 0.6235,  0.6392,  0.6157,  ..., -0.6706, -0.6863, -0.7020],\n",
              "          [ 0.6314,  0.6157,  0.6157,  ..., -0.6941, -0.6784, -0.6706],\n",
              "          [ 0.6392,  0.6000,  0.6078,  ..., -0.6863, -0.7098, -0.6549]],\n",
              " \n",
              "         [[-0.4667, -0.4667, -0.4667,  ...,  0.7804,  0.7412,  0.7490],\n",
              "          [-0.4510, -0.4353, -0.4510,  ...,  0.7647,  0.7647,  0.8039],\n",
              "          [-0.4353, -0.4353, -0.4431,  ...,  0.8275,  0.8196,  0.8196],\n",
              "          ...,\n",
              "          [ 0.5059,  0.5216,  0.4902,  ..., -0.7647, -0.7569, -0.7647],\n",
              "          [ 0.5059,  0.4980,  0.4902,  ..., -0.7882, -0.7412, -0.7490],\n",
              "          [ 0.5216,  0.4824,  0.4902,  ..., -0.7804, -0.7961, -0.7490]],\n",
              " \n",
              "         [[-0.5765, -0.5686, -0.6000,  ...,  0.5529,  0.5137,  0.5529],\n",
              "          [-0.5608, -0.5608, -0.5843,  ...,  0.5608,  0.5529,  0.6078],\n",
              "          [-0.5608, -0.5608, -0.5765,  ...,  0.6392,  0.6235,  0.6392],\n",
              "          ...,\n",
              "          [ 0.3255,  0.3176,  0.3020,  ..., -0.8588, -0.8824, -0.9059],\n",
              "          [ 0.3333,  0.3020,  0.2941,  ..., -0.8824, -0.8588, -0.8667],\n",
              "          [ 0.3255,  0.2706,  0.2863,  ..., -0.8588, -0.8667, -0.8118]]]),\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deepfake Challenge Dataset"
      ],
      "metadata": {
        "id": "XY95mG8gOqdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract dataset zip file from google drive"
      ],
      "metadata": {
        "id": "D8-dmqiJO596"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = 'drive/MyDrive/ComputerVision/datasets/deepfakechallenge/test-task1.zip'\n",
        "extract_path = 'deepfakechallenge/'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "SesbTZA_O3wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize dataset according with label file"
      ],
      "metadata": {
        "id": "gsXbkUFPWBPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "annotations_file = 'drive/MyDrive/ComputerVision/datasets/deepfakechallenge/label-task1.txt'\n",
        "img_dir = extract_path + 'test-task1'\n",
        "dataset_dir = 'deepfakechallenge/test-task1-organized'\n",
        "\n",
        "# Read labels and organize files\n",
        "with open(annotations_file, 'r') as f:\n",
        "    for line in f:\n",
        "        image_name, label = line.strip().split()\n",
        "        label_dir = os.path.join(dataset_dir, f'class_{label}')\n",
        "        if not os.path.exists(label_dir):\n",
        "            os.makedirs(label_dir)\n",
        "        source_path = os.path.join(img_dir, image_name)\n",
        "        dest_path = os.path.join(label_dir, image_name)\n",
        "        shutil.move(source_path, dest_path)\n",
        "\n",
        "print(f\"Images organized into {dataset_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46QXxPi4WE8a",
        "outputId": "8683762b-b779-499a-f37e-7fdef5655183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images organized into deepfakechallenge/test-task1-organized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a wrapper class for this dataset because we have a custom transform that needs also the label value of the sample"
      ],
      "metadata": {
        "id": "olq8pk0ZFIsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChallengeDataset(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root)\n",
        "        self.map = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = super().__getitem__(index)\n",
        "        if self.map is not None:\n",
        "            sample = self.map(image, label)\n",
        "        return sample"
      ],
      "metadata": {
        "id": "DjodrKY-FKUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset in Pytorch"
      ],
      "metadata": {
        "id": "HI3jZ2JtPY5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepfakechallenge_dataset = ChallengeDataset(root='deepfakechallenge/test-task1-organized', transform=dataset_transform)"
      ],
      "metadata": {
        "id": "_DIgdGr9PbxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sample of DeepFakeChallenge dataset"
      ],
      "metadata": {
        "id": "-vXkCR2Fc1ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepfakechallenge_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WU2cAQQX76r",
        "outputId": "530c1bc5-1e9f-4a3b-eaef-3c6220b6db3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vgg16_image': tensor([[[ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],\n",
              "          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],\n",
              "          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],\n",
              "          ...,\n",
              "          [ 2.0263,  2.0263,  2.0263,  ...,  1.6324,  1.6324,  1.6495],\n",
              "          [ 1.9920,  1.9920,  1.9920,  ...,  1.6495,  1.6495,  1.6667],\n",
              "          [ 1.9749,  1.9749,  1.9749,  ...,  1.6667,  1.6667,  1.6838]],\n",
              " \n",
              "         [[ 2.0609,  2.0609,  2.0609,  ...,  2.3060,  2.3060,  2.3060],\n",
              "          [ 2.0609,  2.0609,  2.0609,  ...,  2.3060,  2.3060,  2.3060],\n",
              "          [ 2.0609,  2.0609,  2.0609,  ...,  2.3060,  2.3060,  2.3060],\n",
              "          ...,\n",
              "          [ 1.8158,  1.8158,  1.8158,  ...,  1.0280,  0.9930,  0.9755],\n",
              "          [ 1.7633,  1.7633,  1.7633,  ...,  1.0630,  1.0105,  0.9930],\n",
              "          [ 1.7458,  1.7458,  1.7458,  ...,  1.0630,  1.0280,  1.0105]],\n",
              " \n",
              "         [[-0.6715, -0.6715, -0.6715,  ..., -0.6018, -0.6018, -0.6018],\n",
              "          [-0.6715, -0.6715, -0.6715,  ..., -0.6018, -0.6018, -0.6018],\n",
              "          [-0.6715, -0.6715, -0.6715,  ..., -0.6018, -0.6018, -0.6018],\n",
              "          ...,\n",
              "          [-0.7761, -0.7761, -0.7761,  ...,  0.0605,  0.0605,  0.0779],\n",
              "          [-0.7761, -0.7761, -0.7761,  ...,  0.0779,  0.0779,  0.0953],\n",
              "          [-0.7936, -0.7936, -0.7936,  ...,  0.0779,  0.0953,  0.1128]]]),\n",
              " 'beit_image': tensor([[[ 1.0000,  1.0000,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
              "          [ 1.0000,  1.0000,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
              "          [ 1.0000,  1.0000,  0.9922,  ...,  1.0000,  1.0000,  1.0000],\n",
              "          ...,\n",
              "          [ 0.8980,  0.8980,  0.8980,  ...,  0.7176,  0.7176,  0.7255],\n",
              "          [ 0.8824,  0.8824,  0.8824,  ...,  0.7255,  0.7255,  0.7333],\n",
              "          [ 0.8745,  0.8745,  0.8745,  ...,  0.7333,  0.7333,  0.7412]],\n",
              " \n",
              "         [[ 0.8353,  0.8353,  0.8353,  ...,  0.9451,  0.9451,  0.9451],\n",
              "          [ 0.8353,  0.8353,  0.8353,  ...,  0.9451,  0.9451,  0.9451],\n",
              "          [ 0.8353,  0.8353,  0.8353,  ...,  0.9451,  0.9451,  0.9451],\n",
              "          ...,\n",
              "          [ 0.7255,  0.7255,  0.7255,  ...,  0.3725,  0.3569,  0.3490],\n",
              "          [ 0.7020,  0.7020,  0.7020,  ...,  0.3882,  0.3647,  0.3569],\n",
              "          [ 0.6941,  0.6941,  0.6941,  ...,  0.3882,  0.3725,  0.3647]],\n",
              " \n",
              "         [[-0.4902, -0.4902, -0.4902,  ..., -0.4588, -0.4588, -0.4588],\n",
              "          [-0.4902, -0.4902, -0.4902,  ..., -0.4588, -0.4588, -0.4588],\n",
              "          [-0.4902, -0.4902, -0.4902,  ..., -0.4588, -0.4588, -0.4588],\n",
              "          ...,\n",
              "          [-0.5373, -0.5373, -0.5373,  ..., -0.1608, -0.1608, -0.1529],\n",
              "          [-0.5373, -0.5373, -0.5373,  ..., -0.1529, -0.1529, -0.1451],\n",
              "          [-0.5451, -0.5451, -0.5451,  ..., -0.1529, -0.1451, -0.1373]]]),\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataloader"
      ],
      "metadata": {
        "id": "vY1ebtI8Pk1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "challenge_dataloader = DataLoader(deepfakechallenge_dataset, batch_size=64, shuffle=False, num_workers=8, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "BMp2paEVPnI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Routine"
      ],
      "metadata": {
        "id": "QxH8V29OO18F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize lists to hold true and predicted labels\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "\n",
        "  # Disable gradient computation for evaluation\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        vgg16_images = batch['vgg16_images']\n",
        "        beit_images = batch['beit_images']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        #data to device\n",
        "        vgg16_images = vgg16_images.to(device)\n",
        "        beit_images = beit_images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get the model's predictions\n",
        "        outputs = model(vgg16_images, beit_images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Store the true labels and predicted labels\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(all_labels, all_preds)\n",
        "  print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "  # Calculate precision, recall, and F1-score\n",
        "  precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "  recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "  f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "  print(f'Precision: {precision * 100:.2f}%')\n",
        "  print(f'Recall: {recall * 100:.2f}%')\n",
        "  print(f'F1-Score: {f1 * 100:.2f}%')\n",
        "\n",
        "  # Print the classification report\n",
        "  print('\\nClassification Report:')\n",
        "  print(classification_report(all_labels, all_preds))\n",
        "\n",
        "  # Compute and print the confusion matrix\n",
        "  conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "  print('\\nConfusion Matrix:')\n",
        "  print(conf_matrix)"
      ],
      "metadata": {
        "id": "NsszD41UW9Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Model with FFHQ"
      ],
      "metadata": {
        "id": "tsB1Eq4LalRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, ffhq_dataloader)"
      ],
      "metadata": {
        "id": "YIu1tBtPKrUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce434130-8925-4b97-abd0-74744f5e2365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/547 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 547/547 [56:27<00:00,  5.82s/it]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 547/547 [56:27<00:00,  6.19s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.50%\n",
            "Precision: 100.00%\n",
            "Recall: 90.50%\n",
            "F1-Score: 95.01%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     35000\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91     35000\n",
            "   macro avg       0.50      0.45      0.48     35000\n",
            "weighted avg       1.00      0.91      0.95     35000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[31675  3325]\n",
            " [    0     0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluating Model with DeepfakeChallenge"
      ],
      "metadata": {
        "id": "OfK1UkhLarc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, challenge_dataloader)"
      ],
      "metadata": {
        "id": "s8Lrk8muauUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a28e4e-081d-46e7-b515-3c7462c70d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/110 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 110/110 [02:18<00:00,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 64.79%\n",
            "Precision: 77.59%\n",
            "Recall: 64.79%\n",
            "F1-Score: 66.32%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.86      0.58      2000\n",
            "           1       0.91      0.56      0.70      5000\n",
            "\n",
            "    accuracy                           0.65      7000\n",
            "   macro avg       0.68      0.71      0.64      7000\n",
            "weighted avg       0.78      0.65      0.66      7000\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1722  278]\n",
            " [2187 2813]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}